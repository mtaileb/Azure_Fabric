{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from pyspark.sql.functions import unix_timestamp, date_format, col, when\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.feature import RFormula\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "Exécutez les lignes suivantes pour créer un DataFrame Spark en collant le code dans une nouvelle cellule.\n",
        "Cette étape récupère les données via l'API Open Datasets. L'extraction de toutes ces données génère environ\n",
        "1,5 milliard de lignes.\n",
        "Selon la taille de votre pool Apache Spark serverless, il est possible que les données brutes soient trop\n",
        "volumineuses ou que leur exploitation prenne trop de temps. Vous pouvez filtrer ces données pour en réduire\n",
        "le volume. L'exemple de code suivant utilise start_date et end_date pour appliquer un filtre qui retourne un\n",
        "seul mois de données.\n",
        "'''\n",
        "from azureml.opendatasets import NycTlcYellow\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "\n",
        "end_date = parser.parse('2018-05-08 00:00:00')\n",
        "start_date = parser.parse('2018-05-01 00:00:00')\n",
        "\n",
        "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
        "filtered_df = spark.createDataFrame(nyc_tlc.to_pandas_dataframe())\n",
        "\n",
        "\n",
        "# Le code suivant réduit le jeu de données à environ 2 000 lignes\n",
        "sampled_taxi_df = filtered_df.sample(True, 0.001, seed=1234)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.7936901Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9296253Z",
              "parent_msg_id": "205b245a-8855-42cf-9c2c-22d379554d38"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Le code suivant offre deux façons d’afficher les données. La première est basique. La deuxième offre une\n",
        "expérience de grille beaucoup plus riche ainsi que la possibilité de visualiser les données sous forme graphique.\n",
        "'''\n",
        "#sampled_taxi_df.show(5)\n",
        "display(sampled_taxi_df)\n",
        "\n",
        "'''\n",
        "    Selon la taille du jeu de données généré et la nécessité ou non d’expérimenter ou d’exécuter le notebook\n",
        "    plusieurs fois, vous pouvez mettre en cache le jeu de données localement dans l’espace de travail.\n",
        "    Il existe trois façons d'effectuer une mise en cache explicite :\n",
        "        Enregistrez le DataFrame localement en tant que fichier.\n",
        "        Enregistrez le DataFrame en tant que table ou vue temporaire.\n",
        "        Enregistrez le DataFrame en tant que table permanente.\n",
        "\n",
        "Les deux premières approches sont incluses dans les exemples de code suivants.\n",
        "'''\n",
        "sampled_taxi_df.createOrReplaceTempView(\"nytaxi\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.7958567Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9304498Z",
              "parent_msg_id": "d2e3091e-d282-4812-b112-2adb66b8625e"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dans le code suivant, vous effectuez quatre classes d’opérations :\n",
        "\n",
        "    suppression des valeurs hors norme ou incorrectes par filtrage ;\n",
        "    suppression des colonnes superflues ;\n",
        "    création de colonnes dérivées des données brutes pour un assurer un fonctionnement plus efficace du modèle. Cette opération est parfois appelée caractérisation.\n",
        "    étiquetage. Comme vous procédez à une classification binaire (y aura-t-il ou non un pourboire à l’issue d’une course donnée), il est nécessaire de convertir le montant du pourboire en valeur 0 ou 1.\n",
        "'''\n",
        "taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
        "                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
        "                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
        "                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
        "                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
        "                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
        "                                )\\\n",
        "                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
        "                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
        "                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
        "                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
        "                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
        "                                & (sampled_taxi_df.rateCodeId <= 5)\n",
        "                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
        "                                )\n",
        "\n",
        "\n",
        "# Vous effectuez ensuite une deuxième passe sur les données pour ajouter les caractéristiques finales.\n",
        "taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
        "                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
        "                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
        "                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
        "                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
        "                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
        "                                                .otherwise(0).alias('trafficTimeBins')\n",
        "                                              )\\\n",
        "                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.8300186Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9310831Z",
              "parent_msg_id": "57045fe4-1c2d-447c-b320-fff5badd3af8"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "La dernière tâche consiste à convertir les données étiquetées en format analysable par régression logistique.\n",
        "L'entrée dans un algorithme de régression logistique doit être un jeu de paires de vecteurs étiquette/caractéristique,\n",
        "où le vecteur caractéristique est un vecteur de nombres qui représentent le point d'entrée.\n",
        "\n",
        "Vous devez donc convertir les colonnes catégoriques en nombres. Plus précisément, vous devez convertir les colonnes\n",
        "trafficTimeBins et weekdayString en représentations entières. Cette conversion peut être effectuée en suivant plusieurs\n",
        "approches. L'exemple ci-dessous suit l'approche OneHotEncoder, qui est courante.\n",
        "'''\n",
        "sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\")\n",
        "en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\")\n",
        "sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\")\n",
        "en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\")\n",
        "\n",
        "# Cette action génère un nouveau DataFrame dont les colonnes sont toutes dans un format adapté à l’entraînement d’un modèle\n",
        "encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.8308372Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9314966Z",
              "parent_msg_id": "128a66e9-eff3-4314-bf17-81ff6826b465"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "La première tâche consiste à diviser le jeu de données en un jeu d’entraînement et un jeu de test ou de validation.\n",
        "La division est ici arbitraire. Faites des essais avec différents paramètres de division pour voir s’ils ont un effet sur\n",
        "le modèle.\n",
        "'''\n",
        "# Decide on the split between training and testing data from the DataFrame\n",
        "trainingFraction = 0.7\n",
        "testingFraction = (1-trainingFraction)\n",
        "seed = 1234\n",
        "# Split the DataFrame into test and training DataFrames\n",
        "train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.8341787Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9317976Z",
              "parent_msg_id": "c5d34c82-991b-4d70-8e48-2c4df8d58d10"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Maintenant qu’il existe deux DataFrames, la tâche suivante consiste à créer la formule du modèle et à l’exécuter sur le DataFrame d’entraînement. Vous pouvez ensuite effectuer une validation par rapport au DataFrame de test. Faites des essais avec différentes versions de la formule du modèle pour voir l’impact de différentes combinaisons.\n",
        "'''\n",
        "## Create a new logistic regression object for the model\n",
        "logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
        "\n",
        "## The formula for the model\n",
        "classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
        "\n",
        "## Undertake training and create a logistic regression model\n",
        "lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
        "\n",
        "## Saving the model is optional, but it's another form of inter-session cache\n",
        "datestamp = datetime.now().strftime('%m-%d-%Y-%s')\n",
        "fileName = \"lrModel_\" + datestamp\n",
        "logRegDirfilename = fileName\n",
        "lrModel.save(logRegDirfilename)\n",
        "\n",
        "## Predict tip 1/0 (yes/no) on the test dataset; evaluation using area under ROC\n",
        "predictions = lrModel.transform(test_data_df)\n",
        "predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
        "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
        "\n",
        "# La sortie devrait être : Area under ROC = 0.9779470729751403\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.8750818Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9320792Z",
              "parent_msg_id": "7241585b-88f4-404c-8843-3576683873d6"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Vous pouvez désormais construire une visualisation finale pour faciliter l’examen des résultats de ce test.\n",
        "La courbe ROC est une façon d’examiner les résultats.\n",
        "'''\n",
        "## Plot the ROC curve; no need for pandas, because this uses the modelSummary object\n",
        "modelSummary = lrModel.stages[-1].summary\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(modelSummary.roc.select('FPR').collect(),\n",
        "         modelSummary.roc.select('TPR').collect())\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "cancelled",
              "queued_time": "2025-10-05T18:06:04.8767091Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2025-10-05T18:06:04.9323854Z",
              "parent_msg_id": "a766c4e6-77b5-437f-ae27-64b3c1d60dc6"
            },
            "text/plain": "StatementMeta(, , -1, Cancelled, , Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}